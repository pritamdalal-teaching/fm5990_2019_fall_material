{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 09 - Grouping and Aggregating - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real power of data analysis with `DataFrames` comes into focus when we start utilizing the `.groupby()` and `.agg()` methods.  This is known as *grouping* and *aggegregating*.\n",
    "\n",
    "Talking about grouping in the abstract can be confusing; I think it's best to see grouping in action by doing meaningful calculations.\n",
    "\n",
    "The purpose of this tutorial is to introduce grouping and aggregation by way of following finance task: calculating monthly returns and volatilies for several assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the packages that we will need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import numpy as np\n",
    "##> import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading-In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis we will be performing will be on the set of of December 2018 prices for `SPY`, `IWM`, `QQQ`, `DIA`.\n",
    "\n",
    "Let's read that data in from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf = pd.read_csv(\"../data/index_etf_dec_2018.csv\")\n",
    "##> df_etf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Use `DataFrame` masking to isolate all the `QQQ` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Daily Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bit of code calculates the daily return for each day and each symbol.  It's similar to the code we used in a previous tutorial except there is some logic to deal with the fact that there are different symbols in the same data set.  For the purposes of this tutorial you can simply run the cod.  If you are curious about it, I encourage you to analyze it on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_etf['return'] = np.nan\n",
    "for ix in range(1, df_etf.shape[0]):\n",
    "    \n",
    "    # grabbing symbols from df_etf\n",
    "    curr_sym = df_etf.at[ix, 'symbol']\n",
    "    prev_sym = df_etf.at[ix-1, 'symbol']\n",
    "    \n",
    "    # grabbling prices from df_etf\n",
    "    curr_px = df_etf.at[ix, 'close']\n",
    "    prev_px = df_etf.at[ix-1, 'close']\n",
    "    \n",
    "    # calculating return\n",
    "    if curr_sym == prev_sym:\n",
    "        df_etf.at[ix, 'return'] = (curr_px / prev_px) - 1\n",
    "\n",
    "df_etf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first date in our data set is November 30, 2018.  As a simple sanity check, let's utilize `DataFrame` masking to make sure that all those returns are set as `NaN`, since the return is not defined on the first day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf[df_etf.date == '2018-11-30']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sanity check looks good.\n",
    "\n",
    "We'll now proceed to calculating monthly returns and volatility for each of the ETFs in our data set.  This amounts to first grouping on `symbol`, and then aggregating the `returns`.  \n",
    "\n",
    "Let's start with monthly return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Return for Each `symbol`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at our data set, we now have daily returns for each day, for each of the four ETFs.  Given a bit of time, you could probably come up with a `for` loop to iterate through the `DataFrame` and produce the monthly returns for each ETF (in fact, you could modify the returns `for` loop from the previous section to do just that).  As a one-off solution, that would be fine.  But grouping and aggregating are such ubiquitous operations, that it would be inconvenient to have write a `for`-loop every time.\n",
    "\n",
    "\n",
    "Let's begin by calculating the daily growth factor in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf['growth_factor'] = 1 + df_etf['return']\n",
    "##> df_etf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the monthly growth factor is the product of the daily growth factors.  Here is a way to write all that logic in a single line using `.groupby()` and `.agg()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_factor = \\\n",
    "##>     df_etf.groupby(['symbol'])['growth_factor'].agg([np.prod]).reset_index()\n",
    "##> df_grouped_factor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `pandas` isn't very sophisticated about the name that it gives to the column that stores the aggregation calculation column.  It just gave it the name `prod`, which the name of the function that was used in the aggregation calculation.  Let's make `df_grouped_factor` a bit more readable by renaming that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_factor.rename(columns={'prod': 'monthly_factor'}, inplace=True)\n",
    "##> df_grouped_factor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, recall that the monthly return is gotten by subracting one from the monthly growth factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_factor['monthly_return'] = df_grouped_factor['monthly_factor'] - 1\n",
    "##> df_grouped_factor[['symbol', 'monthly_return']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Volatility for Each `symbol`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the (realized/historical) volatility for each of the ETFs.  Recall that volatility is the standard deviation of the daily returns.  If we were to do this in a brute force manner, we could use dataframe masking to separate out the returns for each of the four ETFs, and then calculate four separate standarde deviations.\n",
    "\n",
    "However, once again with the power of `.groupby()` and `.agg()` we can do all of this with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_vol = \\\n",
    "##>     df_etf.groupby(['symbol'])['return'].agg([np.std]).reset_index()\n",
    "##> \n",
    "##> df_grouped_vol\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's rename our aggregation column to something more descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_vol.rename(columns={'std':'daily_vol'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have calculated is a daily volatility, but when practitioners talk about volatility, the typically annualize it.  A daily volatility is annualized by multiplying by $\\sqrt{252}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_vol['ann_vol'] = df_grouped_vol['daily_vol'] * np.sqrt(252)\n",
    "##> df_grouped_vol\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge** Use `groupby()` and `.agg()` to calculated the average daily return for each of the ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Reading\n",
    "\n",
    "*PDSH* - 3.8 - Aggregation and Grouping\n",
    "\n",
    "*Python for Data Analysis (McKinney)* - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations\n",
    "\n",
    "*Options, Futures, and Other Derivatives (Hull)* - Chapter 15 (pp 325-329) The Black-Scholes-Merton Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

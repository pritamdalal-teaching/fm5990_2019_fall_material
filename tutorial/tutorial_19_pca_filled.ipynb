{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 19 - Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a *supervised* learning problem we, seek to predict a label from a set of features.  If the label we are predicting is continuous, this is known as a regression problem.  If the label is discrete, this is known as a classification problem.\n",
    "\n",
    "We now turn our attention to *unsupervised* learning.  The goal of unsupervised learning is no longer the prediction of a label. Rather we seek to seek to discover interesting or useful things about the features themselves.  Unsupervised learning is more subjective, and therefore more difficult, than supervised learning.  Determining what is *interesting* is quite subjective, and it is harder to assess how successfully you have discovered these interesting things.\n",
    "  \n",
    "Unspervised learning is often used in the exploratory data analysis process, or as a data preprocessing step to a supervised learning problem.\n",
    "\n",
    "This tutorial is an intuitive introduction to principal components analysis (PCA), which is an technique used to reduced the dimensionality of a set of features.  In particular, we apply this technique to our VIX term structure data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import numpy as np\n",
    "##> import pandas as pd\n",
    "##> import seaborn as sns\n",
    "##> import sklearn\n",
    "##> import matplotlib.pyplot as plt\n",
    "##> %matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read-in the same data set that we used in the k-nearest-neighbors tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_vix = pd.read_csv('../data/vix_knn.csv')\n",
    "##> df_vix = df_vix[df_vix.trade_date > '2011-01-03'] #removing the first row of NaNs\n",
    "##> df_vix.drop(['spy_ret'], axis=1, inplace=True) # dropping the returns (label) column\n",
    "##> df_vix.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this data set represents the daily changes in various points in the implied volatility term structure.  We'll also be interested in the absolute levels of the VIX term structure as well, so let's import that data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_vix_ts = pd.read_csv('../data/vix_term_structure.csv')\n",
    "##> df_vix_ts.drop(['spy_ret'], axis=1, inplace=True) # dropping returns column\n",
    "##> df_vix_ts.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Term Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to think of both `df_vix` and `df_vix_ts` as living in Euclidean space.  However, they mostly live in certain specific subspaces of Euclidean space, and it's the goal of PCA to give this observation a more specific meaning.  \n",
    "\n",
    "Before we dive into the specifics of PCA, let's develop an intuition for what a typical VIX term structure looks like, and what kind of daily changes are typical in the term structure.\n",
    "\n",
    "The first stylized fact about the VIX term structre is that it is usually monotonically increasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> cond1 = (df_vix_ts.vix_009 <= df_vix_ts.vix_030)\n",
    "##> cond2 = (df_vix_ts.vix_030 <= df_vix_ts.vix_090)\n",
    "##> cond3 = (df_vix_ts.vix_090 <= df_vix_ts.vix_180)\n",
    "##> \n",
    "##> df_vix_ts[cond1 & cond2 & cond3].shape[0] / df_vix_ts.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that about 70% of days in our data set have an unpwardly sloping term struture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Find the percentage of days in which the 9-day VIX is greater than the 180-day VIX (we'll refer to this as an inverted VIX term structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Term-Structure Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform a similar analysis on the daily VIX changes, which are stored in `df_vix`.  The following code shows that on 75% of days, all the point on the VIX term structure rise or fall together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # decreasing\n",
    "##> cond1 = (df_vix.vix_009 <= 0) \n",
    "##> cond2 = (df_vix.vix_030 <= 0)\n",
    "##> cond3 = (df_vix.vix_090 <= 0)\n",
    "##> cond4 = (df_vix.vix_180 <= 0)\n",
    "##> \n",
    "##> # increasing\n",
    "##> cond5 = (df_vix.vix_009 >= 0) \n",
    "##> cond6 = (df_vix.vix_030 >= 0)\n",
    "##> cond7 = (df_vix.vix_090 >= 0)\n",
    "##> cond8 = (df_vix.vix_180 >= 0)\n",
    "##> \n",
    "##> \n",
    "##> cond = (cond1 & cond2 & cond3 & cond4) | (cond5 & cond6 & cond7 & cond8)\n",
    "##> \n",
    "##> \n",
    "##> df_vix[cond].shape[0] / df_vix.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also reflected in the strong positive correlations that we can observe in the following pair plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> sns.pairplot(df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Our Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize our findings from above:\n",
    "\n",
    "1. About 70% of the time the VIX term structure is monotonically increasing.\n",
    "\n",
    "2. About 10% of the time the VIX term structure is inverted with 9-day greater than 180-day.\n",
    "\n",
    "3. On 75% of days, all points on the term structure either rise or fall together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** If you were to construct a forecasting model for the VIX, would your model assert that all points of the term structure move completely independently of one another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to give an intuitive introduction to PCA, and how to implement it in `sklearn`.  We are not going to get into the mathematical specifics, which involve a fair amount of linear algebra.  \n",
    "\n",
    "Instead, here is an overview of the essential ideas:\n",
    "\n",
    "1. In PCA we are trying to understand a $n$ sized set of $p$-dimensional features. Each of the $n$ feature observations is a vector of $p$ numbers.  It is usually best to scale the features so that each component has mean zero and standard deviation 1.\n",
    "\n",
    "2. We can think of our feature set as a collection of point, living in $p$-dimensional euclidean space ($R^{p}$), with the points being centered around the origin.\n",
    "\n",
    "3. It is often the case that there is some $q$ that is smaller than $p$, such that most of the variation of the features can be described by a $q$ dimensional linear subspace of ${R}^p$.\n",
    "\n",
    "4. PCA is a technique for finding the basis of this $q$ dimensional subspace.\n",
    "\n",
    "5. The first element of this basis is the vector (which defines a line that goes through the origin) that is collectively closest to all of the feature observations.  This is conceptually similar to fitting a regression line.  This vector is called the *first principal component*.  It has the property that if we project our feature observation onto that vector, the sample variance of the projections is maximized.\n",
    "\n",
    "6. The second element of this basis is called the *second principal* component.  It is found by considering all vectors which are perpendicular to the first principle component.  Among these perpendicular vectors, the second component is the one which bests fits the the feature observations.  It has the property that if we project our feature observations onto that vector, then we maximize the sample variance of the projections (among all vectors that are perpendicular to the first principle component).\n",
    "\n",
    "7. We repeat this algorithm until we arrive at $q$ principle components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a PCA on the VIX Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform a PCA on our `df_vix` data set using **sklearn**.\n",
    "\n",
    "We'll begin by normaling our features so that they have mean zero and unity standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.preprocessing import scale\n",
    "##> \n",
    "##> # feature selection\n",
    "##> X = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\n",
    "##> \n",
    "##> # scaling fetures\n",
    "##> Xs = scale(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll instantiate our pca model and then fit for $q = 3$ components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.decomposition import PCA\n",
    "##> pca = PCA(n_components=3)\n",
    "##> pca.fit(Xs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the actual principal components by printing the `.components` property our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(pca.components_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what percentage of the overall variation is explained by each of the components, we print the `.explained_variance_ratio` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(pca.explained_variance_ratio_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that 95% the total variance is explained by the first component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Try refitting the PCA with `n_components=4`.\n",
    "\n",
    "1. Are the first three components the same as before?\n",
    "\n",
    "1. What is the total amount of variance explained by the four componets?\n",
    "\n",
    "1. What happens when you set `n_components=5` and try to refit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the First Three Principal Components: Level, Slope, Curvature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again look at the first three principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.decomposition import PCA\n",
    "##> pca = PCA(n_components=3)\n",
    "##> pca.fit(Xs)\n",
    "##> print(pca.components_)\n",
    "##> print(pca.explained_variance_ratio_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These components have a very intuitive interpration (which is not always the case in PCA):\n",
    "\n",
    "1. **Level:** the first component represents parallel shifts in the VIX term structure - all points increasing or all decreasing.\n",
    "    - 95% of total variance explained\n",
    "    - this means that all the points of the term structure are strongly positively correlated\n",
    "    - we can see this in our pair-plots above, as well as the fact that 75% of days had all points going up or all points going down\n",
    "    \n",
    "    \n",
    "2. **Slope:** the second component represents the front of the curve going up, and the back of the curve going down.\n",
    "    - explains 4% of total variance\n",
    "    - recall that about 70% of days had an upward sloping term structure, and 10% of all days had a downward sloping term structure\n",
    "    - in order for the term structure to go from upward sloping to downward sloping, the curve would need this kind of a move (most likely in conjunction with a parallel shift)\n",
    "    \n",
    "    \n",
    "3. **Curvature:** the third component represnts changes in curvature - moving from more concave to more convex.\n",
    "    - 1% of total variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
